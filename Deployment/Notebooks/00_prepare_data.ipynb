{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\r\n",
        "\r\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "data_lake_account_name = ''\n",
        "file_system_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_complaints = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/complaints.csv\",header=True,escape ='\"',multiLine=True)\n",
        "df_complaints = df_complaints.withColumnRenamed(\"Sub-product\", \"subproduct\")\\\n",
        "                            .withColumnRenamed(\"Sub-issue\", \"subissue\")\n",
        "df_complaints.write.mode(\"overwrite\").saveAsTable(\"default.complaints_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "from datetime import date, datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df_names = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/fictitious_customer_names.csv\",header=True)\n",
        "df_names = df_names.select(\"*\", concat(col(\"FirstName\"),lit(\".\"),col(\"LastName\"),lit(\"@contoso.com\")).alias(\"email\"))\n",
        "df_names = df_names.withColumn('email', lower(regexp_replace(col(\"email\"), \" \", \"\")))\n",
        "\n",
        "df_names = df_names.withColumn(\"created_date\", lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
        "\n",
        "df_names = df_names.select('Name','created_date','email')\n",
        "cols = ['name','created_date','email']\n",
        "df_names = df_names.toDF(*cols)\n",
        "\n",
        "#get count of complaints\n",
        "df_complaint_count = spark.sql('''select count(*) as ccount from default.complaints_data''').toPandas()\n",
        "ccount = df_complaint_count['ccount'].iloc[0]\n",
        "\n",
        "\n",
        "#use fictitious names to duplicate and get names for all complaints\n",
        "df_names = df_names.toPandas()\n",
        "df_names = pd.DataFrame(df_names.values.repeat(ccount/df_names.shape[0] +1,  axis=0), columns=df_names.columns)\n",
        "df_names = df_names.head(ccount)\n",
        "df_names = df_names.sample(frac= 1)\n",
        "\n",
        "df_names = spark.createDataFrame(df_names)\n",
        "df_names = df_names.coalesce(1).withColumn(\"customer_id\", monotonically_increasing_id())\n",
        "\n",
        "\n",
        "df_names.write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/customers/')\n",
        "df_names.write.mode(\"overwrite\").saveAsTable(\"default.customers\")\n",
        "\n",
        "\n",
        "#create customer and complaint links\n",
        "sql_statement = '''select customer_id from default.customers'''\n",
        "df_cust = spark.sql(sql_statement).toPandas()\n",
        "\n",
        "sql_statement = '''select complaint_id from default.complaints_data'''\n",
        "df_complaints = spark.sql(sql_statement).toPandas()\n",
        "\n",
        "df_cust_compl_links = pd.concat([df_cust, df_complaints], axis=1)\n",
        "\n",
        "\n",
        "#write to ADLS and also save as spark table\n",
        "df_cust_compl_links_sp = spark.createDataFrame(df_cust_compl_links) \n",
        "df_cust_compl_links_sp.write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/customers_complaints/')\n",
        "df_cust_compl_links_sp.write.mode(\"overwrite\").saveAsTable(\"default.customers_complaints\")\n",
        "\n",
        "#create employee data\n",
        "df_emp = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/fictitious_employee_names.csv\",header=True)\n",
        "df_emp = df_emp.select(\"*\", concat(col(\"FirstName\"),lit(\".\"),col(\"LastName\"),lit(\"@contoso.com\")).alias(\"email\"))\n",
        "df_emp = df_emp.withColumn('email', lower(regexp_replace(col(\"email\"), \" \", \"\")))\n",
        "df_emp = df_emp.withColumn(\"employee_id\", monotonically_increasing_id())\n",
        "df_emp = df_emp.withColumn(\"created_date\", current_date())\n",
        "\n",
        "\n",
        "df_emp = df_emp.withColumn(\"department\",array(lit(\"Credit Reporting\"),\n",
        "                                        lit(\"Debt Collection\"),lit(\"Banking Services\"),\n",
        "                                        lit(\"Card Services\"),lit(\"Loans\")\n",
        "                                    ).getItem((rand()*5).cast(\"int\")))\n",
        "\n",
        "\n",
        "window = Window.partitionBy(df_emp['department']).orderBy(df_emp['employee_id'].desc())\n",
        "df_emp = df_emp.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 2)\n",
        "df_emp = df_emp.select('Name','created_date','email','department','employee_id')\n",
        "\n",
        "cols = ['name','created_date','email','department','employee_id']\n",
        "df_emp = df_emp.toDF(*cols)\n",
        "\n",
        "df_emp = df_emp.coalesce(1).withColumn(\"employee_id\", monotonically_increasing_id())\n",
        "\n",
        "#write to ADLS and also save as spark table\n",
        "\n",
        "df_emp.write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/employees/')\n",
        "df_emp.write.mode(\"overwrite\").saveAsTable(\"default.employees\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# # upload seed data to cosmosdb for PowerApp\n",
        "df_emp = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/employees/\",header=True)\n",
        "\n",
        "df_emp.write.format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"employees\")\\\n",
        "    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "    .mode('overwrite')\\\n",
        "    .save()\n",
        "\n",
        "\n",
        "df_initial_complaints = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/initial_complaints.csv\",header=True)\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "df_initial_complaints = df_initial_complaints.where(col(\"Department\").isin(['Credit Reporting','Debt Collection','Banking Services','Card Services','Loans']))\n",
        "\n",
        "df_initial_complaints.write.format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"complaints\")\\\n",
        "    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "    .mode('overwrite')\\\n",
        "    .save()\n",
        "\n",
        "df_initial_responses = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/data/initial_responses.csv\",header=True)\n",
        "\n",
        "df_initial_responses = df_initial_responses.withColumn('SupportAgent',when(df_initial_responses.SupportAgent != 'Hailey Simmons',df_initial_responses.SupportAgent).otherwise('Hannah Haynes'))\n",
        "\n",
        "df_initial_responses.write.format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"responses\")\\\n",
        "    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "    .mode('overwrite')\\\n",
        "    .save()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "sql_statement = '''select Complaint_Id,Date_received,State,ZIP_code,Product,subproduct,Issue,subissue,Consumer_complaint_narrative from default.complaints_data'''\n",
        "df = spark.sql(sql_statement).toPandas()\n",
        "\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "print(df.shape)\n",
        "df.replace({'Product':\n",
        "             {'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Reporting',\n",
        "              'Debt collection': 'Debt Collection',\n",
        "              'Credit reporting': 'Credit Reporting',\n",
        "              'Credit card': 'Card Services',\n",
        "              'Bank account or service': 'Banking Services',\n",
        "              'Credit card or prepaid card': 'Card Services',\n",
        "              'Student loan': 'Loans',\n",
        "              'Checking or savings account': 'Banking Services',\n",
        "              'Consumer Loan': 'Loans',\n",
        "              'Vehicle loan or lease': 'Loans',\n",
        "              'Money transfer, virtual currency, or money service': 'Banking Services',\n",
        "              'Payday loan, title loan, or personal loan': 'Loans',\n",
        "              'Payday loan': 'Loans',\n",
        "              'Money transfers': 'Banking Services',\n",
        "              'Prepaid card': 'Card Services',\n",
        "              'Other financial service': 'Other',\n",
        "              'Virtual currency': 'Banking Services'}\n",
        "            }, inplace= True)\n",
        "\n",
        "products_list = ['Credit Reporting','Debt Collection','Banking Services','Card Services','Loans']\n",
        "df = df[df['Product'].isin(products_list)]\n",
        "\n",
        "df.columns = ['complaint_id','date_received','state','zipcode','product','subproduct','issue','subissue','complaint']\n",
        "df.complaint = df.complaint.str.lower()\n",
        "\n",
        "\n",
        "# #remove issues that don't have atleast 1000 entries\n",
        "df = df[df.groupby('issue').complaint.transform(len) >= 1000]\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "import string\n",
        "\n",
        "pattern = r\"[{}]\".format(string.punctuation)\n",
        "df['complaint'] = df['complaint'].str.replace(pattern, ' ')\n",
        "\n",
        "df['complaint'] = df['complaint'].str.replace('X+', '')\n",
        "df['complaint'] = df['complaint'].str.replace('\\n', '')\n",
        "df['complaint'] = df['complaint'].str.replace(' +', ' ')\n",
        "df['complaint'] = df['complaint'].str.lower()\n",
        "df['complaint'] = df['complaint'].str.strip()\n",
        "\n",
        "\n",
        "lengths = [len(df.iloc[i]['complaint'].split()) for i in range(len(df))]\n",
        "\n",
        "\n",
        "df = df[[l >= 5 for l in lengths]]\n",
        "\n",
        "#sample to get 2000 rows for each issue\n",
        "df = df.groupby(['issue']).apply(lambda grp: grp.sample(n=2000,replace=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df[['complaint_id', 'date_received', 'state', 'zipcode', 'product','subproduct', 'issue', 'subissue', 'complaint']]\n",
        "\n",
        "df_data_sp = spark.createDataFrame(df)\n",
        "df_data_sp.write.mode(\"overwrite\").saveAsTable(\"default.complaints_data\")\n",
        "\n",
        "df_data_sp.write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/prepareddata/all/')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df, test_size=0.5)\n",
        "\n",
        "#write train data\n",
        "df_train = df_train.dropna().reset_index(drop=True)\n",
        "\n",
        "df_data_sp = spark.createDataFrame(df_train)\n",
        "df_data_sp.write.mode(\"overwrite\").saveAsTable(\"default.complaints_train\")\n",
        "\n",
        "df_data_sp.write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/prepareddata/train/')\n",
        "\n",
        "#write test data\n",
        "df_test = df_test.dropna().reset_index(drop=True)\n",
        "\n",
        "df_data_sp = spark.createDataFrame(df_test)\n",
        "df_data_sp.write.mode(\"overwrite\").saveAsTable(\"default.complaints_test\")\n",
        "\n",
        "df_data_sp.write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/prepareddata/test/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_train['product'].value_counts()\n",
        "df_train['product'].unique()"
      ]
    }
  ]
}